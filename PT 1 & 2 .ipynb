{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: K-NEAREST NEIGHBOR(KNN) ALGO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KNN is used for classification & regression by leveraging nearby data points(neighbors).  \n",
    "__Steps__:\n",
    "> 1. Choose a point to predict.\n",
    "> 2. Find the K-nearest points (K is a predefined constant like 1,  3, 5).\n",
    "> 3. For **classification**: Predict by taking the most common class among the K neighbors.\n",
    "> 4. For **regression**: Predict by averaging the target values of the K neighbors.\n",
    "> 5. **Weighted Prediction**: KNN can also weigh the neighbors' influence based on their distance from the point being predicted.\n",
    "   \n",
    "> Choosing the right distance metric is crucial to the success of KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-means is a related algorithm but it's used for unsupervised learning and clustering.\n",
    "* In K-means, K represents the number of clusters, not neighbors.\n",
    "* It's an iterative algorithm that groups points based on a distance metric until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A}** DISTANCE METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Distance metrics quantify similarity btn data points{algo like KNN}.   \n",
    "> * Data pts closer in distance are more likely to belong to the same class.   \n",
    "> * **Application**: Each dataset column represents a dimension, allowing distance measurement between points in a multi-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MANHATTAN DISTANCE {c=1}  \n",
    "* It measures the distance by traveling along grid axes. {Walking through a city block by block}. \n",
    "* Applied to higher dimensions(3D SPACE)\n",
    "* **Best for grid based problem & works well in high dimension spaces**\n",
    "> EUCLIDEAN DISTANCE {c=2}  \n",
    "* It measures the straight line distance btn 2 points using pythagorean theorem.\n",
    "* **Most Common. Best when shortest path is needed** \n",
    "> MINKOWSKI DISTANCE {c>2, cubic,fifth}  \n",
    "* Generalization of Manhattan & Euclidean Distance\n",
    "* Defined by parameter of c,changes the exponent of the sum of absolute differences.\n",
    "* Applied in ML{in KNN and choice of c}\n",
    "* **Flexible coz it encompasses Manhattan & Euclidean by changing the parameter c**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manhattan Distance\n",
    "A = (2,3,5)\n",
    "B = (1,-1,3)\n",
    "manhattan_dist = sum(abs(A[i] - B[i]) for i in range(3))\n",
    "manhattan_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.58257569495584\n",
      "4.58\n"
     ]
    }
   ],
   "source": [
    "# Euclidean Distance\n",
    "from math import sqrt\n",
    "A = (2,3,5)\n",
    "B = (1,-1,3)\n",
    "euclidean_distance = sqrt(sum((A[i] -B[i])**2 for i in range(3)))\n",
    "print(euclidean_distance)\n",
    "print(f\"{euclidean_distance:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.179339196381232\n",
      "4.18\n"
     ]
    }
   ],
   "source": [
    "# Minkowski Distance\n",
    "import numpy as np\n",
    "A = (2,3,5)\n",
    "B = (1,-1,3)\n",
    "c = 3\n",
    "# Calculate the Minkowski distance btn points A and B\n",
    "# 1. For each dimension, calculate the absolute difference between A[i] and B[i].\n",
    "# 2. Raise that difference to the power of c.\n",
    "# 3. Sum these powered differences.\n",
    "# 4. Take the c-th root of the sum: raise the sum to the power of 1/c).\n",
    "minkowski_distance = np.power(sum(np.abs(A[i] - B[i])**c for i in range(3)),1/c)\n",
    "print(minkowski_distance)\n",
    "print(f\"{minkowski_distance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **B}** K- NEAREST NEIGHBORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> OVERVIEW \n",
    "* It's a supersised learning algo for classification & regression tasks.\n",
    "* It's principle is that smilar data pts are close together and distance metrics help identify similarity.\n",
    "> FIT STAGE\n",
    "* It stores training data with labels without calculating distances.\n",
    "> PREDICTION STAGE\n",
    "* Calculates the distances between the new data pt and every data pt in the training set.\n",
    "* Identifies the closest K pt(neighbours) and assigns a class based on the majority vote among those neighbors.\n",
    "> DISTANCE METRICS\n",
    "* Uses distance metrics like Manhattan, Euclidean or Minkowski depending on the problem\n",
    "> EVALUATING PERFORMANCE\n",
    "* **FOR CLASSIFICATION**: performance is measures using Accuracy, Precision, Recall and F1-Score.\n",
    "* **FOR REGRESSION**: it averages the target values of the K nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **C}** K- NEAREST NEIGHBORS Classifier - {*Used iris dataset*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Fit Method**\n",
    "* Stores training data for later use\n",
    "> **_get_distances Method**\n",
    "* Calculates Euclidean distance btn test point and every training pt.\n",
    "> **_get_k_nearest Method**\n",
    "* Sorts the distances and returns the indices of the k-nearest neightbors\n",
    "> **_get_label_prediction Method**\n",
    "* Finds the most common label among the k-nearest neighbors\n",
    "> **Predict method**\n",
    "* Generates predictions for all test points using above methods\n",
    "> **Testing Method**\n",
    "* Model is tested on Iris dataset and should have an output score around 97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Import relebant libraries\n",
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define KNN Class\n",
    "class KNN:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X_train, y_train):\n",
    "        # store training data and labels\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    def _get_distances(self,x):\n",
    "        # Create an empty list to store distances\n",
    "        distances = []\n",
    "        # enumerate through the training data to calculate distances\n",
    "        for idx,point in enumerate(self.X_train):\n",
    "            dist = euclidean(x,point)\n",
    "            distances.append((idx,dist))\n",
    "        return distances\n",
    "    def _get_k_nearest(self,dists,k):\n",
    "        # sort the distances by the second value in each tuple(distance)\n",
    "        sorted_dists = sorted(dists,key=lambda x:x[1])\n",
    "        # return the first tuples\n",
    "        return sorted_dists[:k]\n",
    "    def _get_label_prediction(self,k_nearest):\n",
    "        # get the labels for the k_nearest neighbors\n",
    "        labels = [self.y_train[idx] for idx,_ in k_nearest]\n",
    "        # count the frequency of each label\n",
    "        counts = np.bincount(labels)\n",
    "        # return the label with the highest frequency\n",
    "        return np.argmax(counts)\n",
    "    def predict(self,X_test,k=3):\n",
    "        # A list to store the predictions\n",
    "        preds = []\n",
    "        # Iterate through all the test points\n",
    "        for x in X_test:\n",
    "            # get distances to all training pts\n",
    "            distances = self._get_distances(x)\n",
    "            # get k_nearest pts\n",
    "            k_nearest = self._get_k_nearest(distances,k)\n",
    "            # predict label based on the nearest neighbors\n",
    "            pred = self._get_label_prediction(k_nearest)\n",
    "            # Append the prediction to the list\n",
    "            preds.append(pred)\n",
    "        # return the predictions for all the test pts\n",
    "        return preds\n",
    "# load Iris dataset\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Instantiate and fit the KNN model\n",
    "knn = KNN()\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# generate predictions for the test set\n",
    "preds = knn.predict(X_test, k=3)\n",
    "\n",
    "#calculate & print the accuracy\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test,preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **D}** Finding Best value for K in KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Optimal K value in KNN: \n",
    "* **A small K (k=1)**: Can lead to *overfitting* whereby the model is too sensitive to small variations\n",
    "* **A large K**: Can lead to *underfitting* whereby the model oversimplifies and misses important patterns\n",
    "* **Odd values for K (k=3,k=5)** help avoid ties in classification\n",
    "* Generally there is no universally best value for K\n",
    "> Iterating to find best K:\n",
    "* Best to different values of K esp the odd numbers\n",
    "* plot error for each K Value.\n",
    "* Choose the K where the error is lowest or it has the highest performance\n",
    "> KNN & Curse of Dimensionality\n",
    "* Due to the curse of dimensionality, KNN struggles with high-dimensional data that has many columns(features)\n",
    "* This means it's inefficient for very large dataset(thousands of columns, millions of rows) as it also grows exponentially with such large dataset. It is time complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/1. Best K value.webp\" alt=\"Best K Value\" width=\"300\" height=\"300\">\n",
    "\n",
    "> A smaller K (like K=1) leads to overfitting, while a larger K may lead to underfitting.    \n",
    "> The optimal K value is where error is lowest as shown by K=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **E}** KNN with Scikit-Learn -Titanic Dataset Complete Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Import Data & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score,accuracy_score,recall_score,f1_score\n",
    "\n",
    "raw_df = pd.read_csv(r\"C:\\Users\\User\\Documents\\Moringa_labs\\PHASE 4\\1.-Phase-4-SUMMARY-\\DATA\\1. titanic.csv\")\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex   Age  SibSp  Parch     Fare  Embarked_Q  Embarked_S\n",
       "0       3    0  22.0      1      0   7.2500           0           1\n",
       "1       1    1  38.0      1      0  71.2833           0           0\n",
       "2       3    1  26.0      0      0   7.9250           0           1\n",
       "3       1    1  35.0      1      0  53.1000           0           1\n",
       "4       3    0  35.0      0      0   8.0500           0           1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping unnecessary columns\n",
    "df = raw_df.drop(columns=[\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\"])\n",
    "# Convert \"sex column\" to binary encoding \n",
    "df['Sex'] = df['Sex'].replace({\"male\":0 , \"female\":1})\n",
    "# Handle missing values in \"Age column\" - fill with median\n",
    "df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())\n",
    "# drop rows with missing \"Embarked\" values\n",
    "df = df.dropna(subset=[\"Embarked\"])\n",
    "# One hot encode Embarked Column\n",
    "df = pd.get_dummies(df, columns= [\"Embarked\"], drop_first=True)\n",
    "# Separate target(\"Survived\") from features\n",
    "labels = df[\"Survived\"]\n",
    "df = df.drop(columns= [\"Survived\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Split Data into Training & Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.815528</td>\n",
       "      <td>1.390655</td>\n",
       "      <td>-0.575676</td>\n",
       "      <td>-0.474917</td>\n",
       "      <td>-0.480663</td>\n",
       "      <td>-0.500108</td>\n",
       "      <td>-0.311768</td>\n",
       "      <td>0.620174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.386113</td>\n",
       "      <td>1.390655</td>\n",
       "      <td>1.550175</td>\n",
       "      <td>-0.474917</td>\n",
       "      <td>-0.480663</td>\n",
       "      <td>-0.435393</td>\n",
       "      <td>-0.311768</td>\n",
       "      <td>0.620174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.386113</td>\n",
       "      <td>-0.719086</td>\n",
       "      <td>-0.120137</td>\n",
       "      <td>-0.474917</td>\n",
       "      <td>-0.480663</td>\n",
       "      <td>-0.644473</td>\n",
       "      <td>-0.311768</td>\n",
       "      <td>0.620174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.587755</td>\n",
       "      <td>-0.719086</td>\n",
       "      <td>-0.120137</td>\n",
       "      <td>-0.474917</td>\n",
       "      <td>-0.480663</td>\n",
       "      <td>-0.115799</td>\n",
       "      <td>-0.311768</td>\n",
       "      <td>0.620174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.815528</td>\n",
       "      <td>1.390655</td>\n",
       "      <td>-1.107139</td>\n",
       "      <td>0.413551</td>\n",
       "      <td>-0.480663</td>\n",
       "      <td>-0.356656</td>\n",
       "      <td>-0.311768</td>\n",
       "      <td>-1.612452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass       Sex       Age     SibSp     Parch      Fare  Embarked_Q  \\\n",
       "0  0.815528  1.390655 -0.575676 -0.474917 -0.480663 -0.500108   -0.311768   \n",
       "1 -0.386113  1.390655  1.550175 -0.474917 -0.480663 -0.435393   -0.311768   \n",
       "2 -0.386113 -0.719086 -0.120137 -0.474917 -0.480663 -0.644473   -0.311768   \n",
       "3 -1.587755 -0.719086 -0.120137 -0.474917 -0.480663 -0.115799   -0.311768   \n",
       "4  0.815528  1.390655 -1.107139  0.413551 -0.480663 -0.356656   -0.311768   \n",
       "\n",
       "   Embarked_S  \n",
       "0    0.620174  \n",
       "1    0.620174  \n",
       "2    0.620174  \n",
       "3    0.620174  \n",
       "4   -1.612452  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize using stdscaler\n",
    "scaler = StandardScaler()\n",
    "# fit the scaler on training data then transform both train and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Convert into a dataframe\n",
    "scaled_df_train = pd.DataFrame(X_train_scaled, columns = df.columns)\n",
    "scaled_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier with default parameter\n",
    "clf = KNeighborsClassifier()\n",
    "# fit classifier to training data\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "# Use trained model to make predictions on the test set\n",
    "test_preds = clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.71\n",
      "Recall Score: 0.73\n",
      "Accuracy Score: 0.79\n",
      "F1 Score: 0.72\n"
     ]
    }
   ],
   "source": [
    "# A function to print key evaluation metrics\n",
    "def print_metrics(y_true, y_pred):\n",
    "    print(f\"Precision Score: {precision_score(y_true,y_pred):.2f}\")\n",
    "    print(f\"Recall Score: {recall_score(y_true,y_pred):.2f}\")\n",
    "    print(f\"Accuracy Score: {accuracy_score(y_true,y_pred):.2f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_true,y_pred):.2f}\")\n",
    "\n",
    "#Evaluate model performance on test set\n",
    "print_metrics(y_test, test_preds )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tune K value (Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best K 17 with F1 Score of 0.7468354430379746\n"
     ]
    }
   ],
   "source": [
    "# Function to find the best k value by iterating over odd values\n",
    "def find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=25):\n",
    "    best_k = min_k\n",
    "    best_score = 0\n",
    "\n",
    "    #iterating over odd values of K\n",
    "    for k in range(min_k, max_k + 1, 2):\n",
    "        clf = KNeighborsClassifier(n_neighbors = k)\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_test)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "\n",
    "        if f1 > best_score: \n",
    "            best_score = f1\n",
    "            best_k = k\n",
    "\n",
    "    print(f\"The Best K {best_k} with F1 Score of {best_score}\")\n",
    "\n",
    "# Find Best K value\n",
    "find_best_k(X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Model Tuning, Pipelines, & Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A}** GridSearchCv  Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Advanced Supervised Learning Techniques\n",
    "Such as:  \n",
    "   *  1. Hyperparameter tuning(GridSearchCV)  \n",
    "   *  2. Machine Learning Pipelines  \n",
    "   *  3. Model Persistence(Saving the model)  \n",
    "> 1. Tuning model hyperparameters with GridsearchCV\n",
    "* **A} Hyperparameter**: are values set before training a model\n",
    "        1. Maximum tree depth in decision trees\n",
    "        2. Nb of neighbors in K-nearest neighbors\n",
    "    * They help balance bias-variance trade-off.\n",
    "        1. bias{underfitting}\n",
    "        2. variance {overfitting}\n",
    "* **B} GridSearchCV**: It automates searching thru combinations of hyperparameters to optimize model performance.  \n",
    "    * It does an exhaustive search and evaluates each model using K-Fold Cross Validation.\n",
    "    * Tuning parameters include: criterion, max_depth, min_samples_split for decision trees\n",
    "    * **Disadvantage**: Time consuming & computationally expensive. We only use the most important hyperparameters.\n",
    "> 2. Machine Learning Pipelines  \n",
    "   \n",
    "  * Pipelines help streamline Ml Workflow.  \n",
    "  * A single Pipeline can handle data transformation (cleaning & encoding), model training and prediction  \n",
    "  * It helps prevent data leakage{where info from test data influences model training}\n",
    "  * Integrating GridSearchCv into a pipeline will automate hyperparameter tuning within a workflow.\n",
    "> 3. Save & Reuse model with Pickle  \n",
    "  * Pickle a model, it'll save to disk space and you don't have to retrain a model each time\n",
    "  * Pickled models can be reused for future predictions, often deployed in production of APIs\n",
    "> Parameter Tuning with GridsearchCV\n",
    "  * Gridsearch allows tuning multiple parameters while searching for the best combination\n",
    "  * It can be resource-extensive esp with complex models and large datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **B}** PIPELINES INTRO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pipelines allow you to combine multiple steps into a single manageable workflow. Steps include:\n",
    "  * data preprocessing\n",
    "  * feature selection\n",
    "  * model training \n",
    "> Pipelines support integration with grid search & cross-validation, streamlining & hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **C}** REFACTORING CODE TO USE PIPELINES "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.** CODE WITHOUT PIPELINES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **PREPROCESSING**: Transform data into suitable ML models\n",
    "* **APPROACH**: The code manually applies preprocessing step in a sequence.\n",
    "* **TRANSFORMER**: {OnehotEncoder,Standardscaler} transform specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B    C    number  number_odd\n",
      "0  1.224745 -0.816497 -0.5  0.000000    0.816497\n",
      "1  1.224745 -0.816497 -0.5  0.597614   -1.224745\n",
      "2 -0.816497  1.224745 -0.5  1.195229    0.816497\n",
      "3 -0.816497  1.224745 -0.5  0.000000    0.816497\n",
      "4 -0.816497 -0.816497  2.0 -1.792843   -1.224745\n"
     ]
    }
   ],
   "source": [
    "# dataset has both categorical & numerical columns\n",
    "example_data = pd.DataFrame([\n",
    "    {\"category\": \"A\", \"number\": 7, \"target\": 1},\n",
    "    {\"category\": \"A\", \"number\": 8, \"target\": 1},\n",
    "    {\"category\": \"B\", \"number\": 9, \"target\": 0},\n",
    "    {\"category\": \"B\", \"number\": 7, \"target\": 1},\n",
    "    {\"category\": \"C\", \"number\": 4, \"target\": 0}\n",
    "])\n",
    "# separate features and target variable\n",
    "example_X = example_data.drop(\"target\", axis=1)\n",
    "example_y = example_data[\"target\"]\n",
    "\n",
    "# one hot encode the category column to convert it to numeric format\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(categories= \"auto\", handle_unknown= \"ignore\", sparse= False)\n",
    "category_encoded = ohe.fit_transform(example_X[[\"category\"]])\n",
    "\n",
    "# Convert the one-hot encoded array to a DataFrame\n",
    "category_encoded_df = pd.DataFrame(category_encoded, columns=ohe.categories_[0], index=example_X.index)\n",
    "\n",
    "# Feature engineering: {number_odd: indicates if value is odd(1) or even(1)}\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "func_transformer = FunctionTransformer(lambda x: x%2)\n",
    "number_odd = func_transformer.fit_transform(example_X[\"number\"])\n",
    "example_X[\"number_odd\"] = number_odd\n",
    "\n",
    "# Combine the encoded category DataFrame with the original DataFrame (without the category column)\n",
    "example_X = example_X.drop(\"category\", axis=1)  # Drop original 'category' column\n",
    "example_X = pd.concat([category_encoded_df, example_X], axis=1)  #concatenate encoded features with numeric features\n",
    "\n",
    "# Standardscaler to normalize numerical data so that all features have a mean 0 and stdev 1\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(example_X)\n",
    "# convert scaled dat back into a dataframe\n",
    "scaled_df = pd.DataFrame(data_scaled, columns=example_X.columns, index=example_X.index)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>number</th>\n",
       "      <th>number_odd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.224745</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.224745</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.597614</td>\n",
       "      <td>-1.224745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.816497</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.195229</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.816497</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.816497</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.792843</td>\n",
       "      <td>-1.224745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B    C    number  number_odd\n",
       "0  1.224745 -0.816497 -0.5  0.000000    0.816497\n",
       "1  1.224745 -0.816497 -0.5  0.597614   -1.224745\n",
       "2 -0.816497  1.224745 -0.5  1.195229    0.816497\n",
       "3 -0.816497  1.224745 -0.5  0.000000    0.816497\n",
       "4 -0.816497 -0.816497  2.0 -1.792843   -1.224745"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FULL CODE FROM GPT FOR DEEP UNDERSTANDING\n",
    "def preprocess_data_without_pipeline(X):\n",
    "    \"\"\"\n",
    "    Manually preprocess data without using a pipeline.\n",
    "    Steps include:\n",
    "    1. Encoding categorical data\n",
    "    2. Feature engineering (adding new features)\n",
    "    3. Scaling all features\n",
    "    \n",
    "    Args:\n",
    "    X (DataFrame): Input data with categorical and numerical columns.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Transformed dataset.\n",
    "    List: A list of transformers used during the process.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List to store transformers (e.g., encoders, scalers)\n",
    "    transformers = []\n",
    "\n",
    "    ### 1. Encoding Categorical Data ###\n",
    "    \n",
    "    # Step 1: Instantiate OneHotEncoder for the 'category' column.\n",
    "    ohe = OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\", sparse=False)\n",
    "    \n",
    "    # Step 2: Apply OneHotEncoder to the 'category' column.\n",
    "    category_encoded = ohe.fit_transform(X[[\"category\"]])\n",
    "    \n",
    "    # Step 3: Convert the encoded array into a DataFrame for readability.\n",
    "    category_encoded = pd.DataFrame(\n",
    "        category_encoded,  # Encoded values\n",
    "        columns=ohe.categories_[0],  # Categorical labels from the encoder\n",
    "        index=X.index  # Keep the original index\n",
    "    )\n",
    "    \n",
    "    # Step 4: Store the encoder in the list of transformers for later use.\n",
    "    transformers.append(ohe)\n",
    "    \n",
    "    # Step 5: Drop the original 'category' column and append the encoded version.\n",
    "    X.drop(\"category\", axis=1, inplace=True)\n",
    "    X = pd.concat([category_encoded, X], axis=1)\n",
    "\n",
    "    ### 2. Feature Engineering ###\n",
    "    \n",
    "    # Step 6: Define a helper function to flag odd numbers (returns 1 if odd, 0 if even).\n",
    "    def is_odd(data):\n",
    "        return data % 2\n",
    "\n",
    "    # Step 7: Create a transformer using FunctionTransformer.\n",
    "    func_transformer = FunctionTransformer(is_odd)\n",
    "\n",
    "    # Step 8: Apply the transformer to the 'number' column to create the 'number_odd' feature.\n",
    "    number_odd = func_transformer.fit_transform(X[\"number\"])\n",
    "    \n",
    "    # Step 9: Store the transformer.\n",
    "    transformers.append(func_transformer)\n",
    "    \n",
    "    # Step 10: Add the new 'number_odd' feature to the dataset.\n",
    "    X[\"number_odd\"] = number_odd\n",
    "\n",
    "    ### 3. Scaling the Data ###\n",
    "    \n",
    "    # Step 11: Instantiate a StandardScaler to standardize the features.\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Step 12: Fit and apply the scaler to the entire dataset.\n",
    "    data_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Step 13: Store the scaler for future use (e.g., inverse transform).\n",
    "    transformers.append(scaler)\n",
    "\n",
    "    # Step 14: Replace the original data with the scaled version, converting back to DataFrame.\n",
    "    X = pd.DataFrame(\n",
    "        data_scaled,  # Scaled values\n",
    "        columns=X.columns,  # Preserve the original column names\n",
    "        index=X.index  # Keep the same index\n",
    "    )\n",
    "\n",
    "    # Return the transformed dataset and the list of transformers used.\n",
    "    return X, transformers\n",
    "\n",
    "# Example usage\n",
    "# Remove target column from the example data for preprocessing\n",
    "example_X = example_data.drop(\"target\", axis=1)\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "result, transformers = preprocess_data_without_pipeline(example_X)\n",
    "\n",
    "# View the transformed dataset\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.** CODE WITH PIPELINES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. PIPELINE CLASS\n",
    "* Helps streamline preporocessing & model steps allow for consistency in transformations\n",
    "* It standardizes features & feeds them to a logistic regression model.\n",
    "> 2. Columntransformer Class\n",
    "* Allow application of distinct transformations to different columns.\n",
    "* eg Apply ohe to categorical data only and not on the numeric data\n",
    "* After transformation pipelines return a numpy array but easily convertible to Dataframe.\n",
    "> 3. FeatureUnion Class\n",
    "* It applies multiple transformations & concatenate their results \n",
    "* e.g A feature to flag whether a number is odd while also encoding categorical data\n",
    "> A complete pipeline would include all the 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler\n",
    "\n",
    "\n",
    "def preprocess_data_with_pipeline(X):\n",
    "    # 1. define the columntransformer for original features encoding\n",
    "    original_features_encoded = ColumnTransformer(\n",
    "        transformers = [\n",
    "            (\"ohe\", OneHotEncoder(categories= \"auto\", handle_unknown= \"ignore\"), [\"category\"])\n",
    "        ],\n",
    "        remainder = \"passthrough\"\n",
    "    )\n",
    "    # 2. create a fn for feature engineering to identify odd and even numbers\n",
    "    def is_odd(data):\n",
    "        return data % 2 # 1 for odd nbs & 0 for even numbers\n",
    "    # 3. Create a column transformer for feature engineering\n",
    "    feature_eng = ColumnTransformer(\n",
    "        transformers = [\n",
    "         # Apply Function transformer to create a new number odd feature\n",
    "            (\"add_number_odd\", FunctionTransformer(is_odd), [\"number\"]) \n",
    "        ],\n",
    "        # drop unspecified columnns in the transformer\n",
    "        remainder =\"drop\" \n",
    "    )\n",
    "    # 4. Combine transformations into a FeatureUnion\n",
    "    feature_union = FeatureUnion(transformer_list = [\n",
    "        # encoded features from the original features transformer\n",
    "        (\"encoded_features\", original_features_encoded),\n",
    "        # Engineered features from feature engineering transformer\n",
    "        (\"engineered_features\", feature_eng)\n",
    "    ])\n",
    "     # 5. Create a pipeline to manage the transformation and scaling\n",
    "    pipe = Pipeline(steps=[\n",
    "        # Apply feature union transformations\n",
    "        (\"feature_union\", feature_union),\n",
    "        # scale the resulting features using standard scaler\n",
    "        (\"scale\", StandardScaler())\n",
    "     ])\n",
    "    # 6. Fit the pipeline to data and transform it\n",
    "    transformed_data = pipe.fit_transform(X)\n",
    "    # 7. Extract category labels for final dataframe creation\n",
    "    encoder = original_features_encoded.named_transformers_[\"ohe\"]\n",
    "    category_labels = encoder.categories_[0]\n",
    "    # 8. Combine transformed data into a dataframe with appropriate column names\n",
    "    all_cols = list(category_labels) + [\"number\", \"number_odd\"]\n",
    "    return pd.DataFrame(transformed_data, columns= all_cols, index = X.index), pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **D}** ENSEMBLE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* They combine multiple models to improve prediction accuracy thus outperforming individual models.\n",
    "* *Ensemble Models* leverage the wisdom of crowds{Average multiple individual estimates} which often yields better results. \n",
    "    * Bootstrap Aggregation{bagging} is introduced here\n",
    "    * Boosting focuses on improving weak models\n",
    "    * Bagging reduces variance by creating multiple models from random subsets of data\n",
    "* *Random Forest* are an ensemble method for decision trees. It uses bagging & subspace sampling to create a forest of decision trees. This improves accuracy over a single tree\n",
    "* *GridSearchCV* tunes decision trees allowing exhaustive parameter search to optimize model performace.\n",
    "* *Gradient Boosting* introduced with the concept of weak learners & powerful ensemble methods like Adaboost and Gradient Boosted Trees.\n",
    "* *XGBOOST* {eXtreme Gradient Boosting} an effective boosting algorithm.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensembles are key to winning kaggle competitions.\n",
    "* **Bagging (Bootstrap Aggregation)**: Creates multiple models using different subsets of data & aggregates their predictions thus reducing variance.\n",
    "* **Tree-based models**: e.g Random Forests & Gradient boosted trees have a resilience against a higher variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **E}** RANDOM FORESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> INTRODUCTION\n",
    "* Random forests use decision trees for both classification and regression tasks.\n",
    "* Random forest provide a balance btn accuracy, interpretability & robustness. Helps work on complex real world problems.\n",
    "* **Main Idea** is to create multiple decision trees with varying inputs to improve prediction accuracy and robustness of the model.\n",
    "* They mitigate overfitting and handle noisy data much better.\n",
    "> MAIN CONCEPTS\n",
    "* **Bagging(Bootstrap Aggregation)**: Train each tree on diff samples of data. 2/3 of data is used to train each tree while rest is left for out of bag{OOB} error estimation.\n",
    "* **Subspace sampling method**: For each tree, a random subset of features is chosen at each node ensuring no single tree over relies on any one feature.\n",
    ">RESILIENCE TO OVERFITTING\n",
    "* Randomness in data & feature sampling ensures the model is resilient to overfitting.\n",
    "* Noisy data in one tree does not affect the overall model's performance coz other trees rely on diff features.\n",
    "> PREDICTION PROCESS\n",
    "* Once all trees have been traines, the algo aggregates prediction from all trees{often using majority voting for classification or Averaging for Regression}\n",
    "> BENEFITS\n",
    "1. **Strong Performance**: Works well on a wide range of Data Science problems\n",
    "2. **Interpretability**: Individual trees are interpretable and it's possible to examine feature importance.\n",
    "> DISADVANTAGES\n",
    "1. **Computational Complexity**: Training many trees is time consuming and the computational cost is high.\n",
    "2. **Memory usage**: Random Forests require significant memory since they need to store every tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **F}** GRADIENT BOOSTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> INTRODUCTION\n",
    "* Gradient boosting helps improve accuracy.\n",
    "* It builds models iteratively with each new model correcting the errors of the previous model.\n",
    "* Iteratively improves model performance by focusing on the hardest examples. \n",
    "* **Adaboost** uses weighted data to adjust training\n",
    "* **Gradient Boosting** leverages gradient descent to minimize errors\n",
    "> WEAK LEARNERS & BOOSTING\n",
    "* **Weak Learners**: Simple models that perform slightly better than random guessing. \n",
    "* Boosting uses weak learners to improve prediction iteratively.  \n",
    "      **Boosting Process**  \n",
    "      1. Train a weak learner  \n",
    "      2. Identify the errors it makes  \n",
    "      3. Train the next learner to focus on those errors  \n",
    "      4. Rpt to create a strong model from many weak ones\n",
    "> UNDERSTANDING Adaboost\n",
    "* **Adaboost**: Trains weak learners on diff subsamples of data adjusting the weights of examples after each iteration.\n",
    "> UNDERSTANDING GRADIENT BOOSTING  \n",
    "* **Gradient Descent**: In gradient boosting, models are trained using gradient descent to minimize loss.\n",
    "* The algo identifies where predictions are wrong, calculates residuals & adjusts parameters.\n",
    "> LEARNING RATE\n",
    "* **Learning Rate(y)**: A scalar that controls how much the modedks parameters are adjusted at each step.\n",
    "* Lowering learning rate make slower but more accurate adjustments.\n",
    "* Higher Learning rate can overshoot optimal values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Boosting vs Random Forests  \n",
    "\n",
    "|SIMILARITIES | DIFFERENCES|\n",
    "|---------|-----------|\n",
    "|* Both methods are ensembles that aggregate the prediction of multiple models  | i) Boosting trees are weak learners ii) Random forest trees are strong learners |\n",
    "| - | i) Boosting uses weighted aggregation of predictions ii) random forests use majority voting | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **G}** XGBOOST (eXtreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> INTRODUCTION\n",
    "* It's the leading gradient oosting algo.\n",
    "* Widely recognized for its speed & performance in ML Tasks esp in Kaggle\n",
    "> WHY IS XGBOOST SPECIAL?\n",
    "* It outperforms other gradient boosting algos thru multiple optimizations:\n",
    "  * *Parallelization*: During tree construction, it uses all CPU cores making it **faster**.\n",
    "  * *Distributed Training*: Can be run across multiple computers allowing for **Scalability**\n",
    "  * *Handling Missing Data* : Automatically manages missing values reducing the need for preprocessing.\n",
    "* These optimizations make it the fastest, most efficient gradient boosting tool available. widely used in real world application.\n",
    "> APPLICATION OF XGBOOST\n",
    "1. **Competitions**:Dominant in Data Science competitions e.g Kaggle due to its top-tier perfirmance in classification tasks.\n",
    "2. **Best-in Class**: Outperforms most algos{except deep learning in some cases} thus making it a go-to choice for many practical applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

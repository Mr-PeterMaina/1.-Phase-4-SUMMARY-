{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: K-NEAREST NEIGHBOR(KNN) ALGO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KNN is used for classification & regression by leveraging nearby data points(neighbors).  \n",
    "__Steps__:\n",
    "> 1. Choose a point to predict.\n",
    "> 2. Find the K-nearest points (K is a predefined constant like 1,  3, 5).\n",
    "> 3. For **classification**: Predict by taking the most common class among the K neighbors.\n",
    "> 4. For **regression**: Predict by averaging the target values of the K neighbors.\n",
    "> 5. **Weighted Prediction**: KNN can also weigh the neighbors' influence based on their distance from the point being predicted.\n",
    "   \n",
    "> Choosing the right distance metric is crucial to the success of KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-means is a related algorithm but it's used for unsupervised learning and clustering.\n",
    "* In K-means, K represents the number of clusters, not neighbors.\n",
    "* It's an iterative algorithm that groups points based on a distance metric until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A}** DISTANCE METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Distance metrics quantify similarity btn data points{algo like KNN}.   \n",
    "> * Data pts closer in distance are more likely to belong to the same class.   \n",
    "> * **Application**: Each dataset column represents a dimension, allowing distance measurement between points in a multi-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MANHATTAN DISTANCE {c=1}  \n",
    "* It measures the distance by traveling along grid axes. {Walking through a city block by block}. \n",
    "* Applied to higher dimensions(3D SPACE)\n",
    "* **Best for grid based problem & works well in high dimension spaces**\n",
    "> EUCLIDEAN DISTANCE {c=2}  \n",
    "* It measures the straight line distance btn 2 points using pythagorean theorem.\n",
    "* **Most Common. Best when shortest path is needed** \n",
    "> MINKOWSKI DISTANCE {c>2, cubic,fifth}  \n",
    "* Generalization of Manhattan & Euclidean Distance\n",
    "* Defined by parameter of c,changes the exponent of the sum of absolute differences.\n",
    "* Applied in ML{in KNN and choice of c}\n",
    "* **Flexible coz it encompasses Manhattan & Euclidean by changing the parameter c**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manhattan Distance\n",
    "A = (2,3,5)\n",
    "B = (1,-1,3)\n",
    "manhattan_dist = sum(abs(A[i] - B[i]) for i in range(3))\n",
    "manhattan_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.58257569495584\n",
      "4.58\n"
     ]
    }
   ],
   "source": [
    "# Euclidean Distance\n",
    "from math import sqrt\n",
    "A = (2,3,5)\n",
    "B = (1,-1,3)\n",
    "euclidean_distance = sqrt(sum((A[i] -B[i])**2 for i in range(3)))\n",
    "print(euclidean_distance)\n",
    "print(f\"{euclidean_distance:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.179339196381232\n",
      "4.18\n"
     ]
    }
   ],
   "source": [
    "# Minkowski Distance\n",
    "import numpy as np\n",
    "A = (2,3,5)\n",
    "B = (1,-1,3)\n",
    "c = 3\n",
    "# Calculate the Minkowski distance btn points A and B\n",
    "# 1. For each dimension, calculate the absolute difference between A[i] and B[i].\n",
    "# 2. Raise that difference to the power of c.\n",
    "# 3. Sum these powered differences.\n",
    "# 4. Take the c-th root of the sum: raise the sum to the power of 1/c).\n",
    "minkowski_distance = np.power(sum(np.abs(A[i] - B[i])**c for i in range(3)),1/c)\n",
    "print(minkowski_distance)\n",
    "print(f\"{minkowski_distance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **B}** K- NEAREST NEIGHBORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> OVERVIEW \n",
    "* It's a supersised learning algo for classification & regression tasks.\n",
    "* It's principle is that smilar data pts are close together and distance metrics help identify similarity.\n",
    "> FIT STAGE\n",
    "* It stores training data with labels without calculating distances.\n",
    "> PREDICTION STAGE\n",
    "* Calculates the distances between the new data pt and every data pt in the training set.\n",
    "* Identifies the closest K pt(neighbours) and assigns a class based on the majority vote among those neighbors.\n",
    "> DISTANCE METRICS\n",
    "* Uses distance metrics like Manhattan, Euclidean or Minkowski depending on the problem\n",
    "> EVALUATING PERFORMANCE\n",
    "* **FOR CLASSIFICATION**: performance is measures using Accuracy, Precision, Recall and F1-Score.\n",
    "* **FOR REGRESSION**: it averages the target values of the K nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **C}** K- NEAREST NEIGHBORS Classifier - {*Used iris dataset*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Fit Method**\n",
    "* Stores training data for later use\n",
    "> **_get_distances Method**\n",
    "* Calculates Euclidean distance btn test point and every training pt.\n",
    "> **_get_k_nearest Method**\n",
    "* Sorts the distances and returns the indices of the k-nearest neightbors\n",
    "> **_get_label_prediction Method**\n",
    "* Finds the most common label among the k-nearest neighbors\n",
    "> **Predict method**\n",
    "* Generates predictions for all test points using above methods\n",
    "> **Testing Method**\n",
    "* Model is tested on Iris dataset and should have an output score around 97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Import relebant libraries\n",
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define KNN Class\n",
    "class KNN:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X_train, y_train):\n",
    "        # store training data and labels\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    def _get_distances(self,x):\n",
    "        # Create an empty list to store distances\n",
    "        distances = []\n",
    "        # enumerate through the training data to calculate distances\n",
    "        for idx,point in enumerate(self.X_train):\n",
    "            dist = euclidean(x,point)\n",
    "            distances.append((idx,dist))\n",
    "        return distances\n",
    "    def _get_k_nearest(self,dists,k):\n",
    "        # sort the distances by the second value in each tuple(distance)\n",
    "        sorted_dists = sorted(dists,key=lambda x:x[1])\n",
    "        # return the first tuples\n",
    "        return sorted_dists[:k]\n",
    "    def _get_label_prediction(self,k_nearest):\n",
    "        # get the labels for the k_nearest neighbors\n",
    "        labels = [self.y_train[idx] for idx,_ in k_nearest]\n",
    "        # count the frequency of each label\n",
    "        counts = np.bincount(labels)\n",
    "        # return the label with the highest frequency\n",
    "        return np.argmax(counts)\n",
    "    def predict(self,X_test,k=3):\n",
    "        # A list to store the predictions\n",
    "        preds = []\n",
    "        # Iterate through all the test points\n",
    "        for x in X_test:\n",
    "            # get distances to all training pts\n",
    "            distances = self._get_distances(x)\n",
    "            # get k_nearest pts\n",
    "            k_nearest = self._get_k_nearest(distances,k)\n",
    "            # predict label based on the nearest neighbors\n",
    "            pred = self._get_label_prediction(k_nearest)\n",
    "            # Append the prediction to the list\n",
    "            preds.append(pred)\n",
    "        # return the predictions for all the test pts\n",
    "        return preds\n",
    "# load Iris dataset\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Instantiate and fit the KNN model\n",
    "knn = KNN()\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# generate predictions for the test set\n",
    "preds = knn.predict(X_test, k=3)\n",
    "\n",
    "#calculate & print the accuracy\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test,preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **D}** Finding Best value for K in KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Optimal K value in KNN: \n",
    "* **A small K (k=1)**: Can lead to *overfitting* whereby the model is too sensitive to small variations\n",
    "* **A large K**: Can lead to *underfitting* whereby the model oversimplifies and misses important patterns\n",
    "* **Odd values for K (k=3,k=5)** help avoid ties in classification\n",
    "* Generally there is no universally best value for K\n",
    "> Iterating to find best K:\n",
    "* Best to different values of K esp the odd numbers\n",
    "* plot error for each K Value.\n",
    "* Choose the K where the error is lowest or it has the highest performance\n",
    "> KNN & Curse of Dimensionality\n",
    "* Due to the curse of dimensionality, KNN struggles with high-dimensional data that has many columns(features)\n",
    "* This means it's inefficient for very large dataset(thousands of columns, millions of rows) as it also grows exponentially with such large dataset. It is time complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/1. Best K value.webp\" alt=\"Best K Value\" width=\"300\" height=\"300\">\n",
    "\n",
    "> A smaller K (like K=1) leads to overfitting, while a larger K may lead to underfitting.    \n",
    "> The optimal K value is where error is lowest as shown by K=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

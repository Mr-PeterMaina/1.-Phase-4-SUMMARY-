{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: RECOMMENDATION SYSTEMS & CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A}** Recommendation systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> INTRODUCTION \n",
    "* A **Recommendation System** predicts user preferences to suggest items they will like.\n",
    "* Help navigate massive inventories e.g Spotify song suggestion.\n",
    "* **Long-tail concept**:\n",
    "    * Many niche products are less popular but still valuable while a few popular products are frequently consumed.\n",
    "    * This concept allow companies to make profits by selling low volumes of hard to find items to many customers instead of only selling large volumes of a reduced nb of popular items.\n",
    "> TYPES OF RECOMMENDATION SYSTEMS\n",
    "1. **Unpersonalized Recommendations**\n",
    "    * These recommendations are based on global trends & are not tailored to any specific users. \n",
    "    * i.e Youtube recommending most popular videos.\n",
    "    * Useful for general content (news) but it's not niche based.\n",
    "2. **Personalized Recommendations**\n",
    "    * Tailored based on a user's profile (past behavior, preferences) & context(browsing history)\n",
    "    * The goal is to predict a relevance score for unseen items & recommend them.\n",
    "> APPROACHES OF RECOMMENDATION SYSTEMS\n",
    "1. **Collaborative Filtering** {CF}\n",
    "    * Recommends items based on past interactions of users {*user-item interaction matrix*}\n",
    "    * It finds similarities btn users or items & predicts preferences. \n",
    "    * The goal is to predict user preferences {numerical rating or Top-N recommendations} based on user behavior{ratings}\n",
    "    * **DISADVANTAGE**\n",
    "        * Cold start problem - it's hard to recommend items without prior user activity.\n",
    "2. **Content-based filtering**{CBF}\n",
    "    * Recommends items based on features of the items themselves.\n",
    "    * i.e if you like sci-fi movies, it recommends more sci-fi movies.  \n",
    "\n",
    "| Pros | Cons |\n",
    "| ---- | ---- |\n",
    "| Offers transparent recommendations based on item similarities | Requires detailed tagging or feature extraction |  \n",
    "\n",
    "\n",
    "> TYPES OF COLLABORATIVE FILTERING\n",
    "1. **User-based Collaborative Filtering**\n",
    "    * Finds users similar to a given user then recommends items that those similar users liked.\n",
    "2. **Item-based Collaborative Filtering**\n",
    "    * Finds items similar to a given item then recommends those items to users who liked the original item.\n",
    "\n",
    "> APPROACHES OF COLLABORATIVE FILTERING\n",
    "1. *Memory-based*:  Uses entire user-item interaction matrix\n",
    "2. *Model-based*: Learns patterns from data using ML Models.  \n",
    "\n",
    "| Memory-based | model-based |\n",
    "| ----  | ---- |\n",
    "| Requires complete data | uses learned abstractions that represent input data |\n",
    "| Doesn't scale well | scales much better |\n",
    "| No pre-computation | Enables pre-computation |\n",
    "| relies on similarity metrics btn users and items | relies on matrix factorization |\n",
    "\n",
    "> MATRIX FACTORIZATION\n",
    "* A key technique in Collaborative filtering.\n",
    "* it decomposes the user-item interaction matrix into two lower-dimensional matrices {user factors & item factors}\n",
    "* These factors capture latent features that explain the interactions.\n",
    "> MATRIX FACTORIZATION TECHNIQUES\n",
    "1. **Singular Value Decomposition (SVD)**\n",
    "    * Decomposes a matrix into 3 matrices: \n",
    "       * where : A=UΣV^T\n",
    "       * U = user features, V = item features &  Σ contains singular values.\n",
    "    * Helps reduce dimensionality & discover patterns in user preferences\n",
    "2. **Alternating Least Square (ALS)**\n",
    "    * Optimizes the factorization by alternating btn fixing user factors & optimizing item factors then vice versa\n",
    "    * Works well with sparse data {commonly found in recommendation systems}   \n",
    "> KEY SIMILARITY METRICS \n",
    "* Cosine similarity  \n",
    "* Euclidean distance  \n",
    "* Pearson Correlation  \n",
    "* Jaccard Index(Binary data)  \n",
    "\n",
    "> EMBEDDINGS\n",
    "* **User & Item Embeddings**: Latent factors rep abstract qualities of users & items  {i.e User preferences for genres }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **B}** COLLABORATIVE FILTERING WITH SVD IN PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.21829477e-01  4.58445949e-02]\n",
      " [ 8.50288016e-01  3.86369035e-01]\n",
      " [-1.91514702e-17 -6.81541027e-19]\n",
      " [-3.88289052e-01  2.35719092e-01]\n",
      " [-2.77549248e-01  8.90535654e-01]] [ 3.89366418 10.99269663] [[ 8.63729488e-01 -7.45693935e-17 -5.03955724e-01]\n",
      " [ 5.03955724e-01 -7.49197375e-18  8.63729488e-01]]\n",
      " \n",
      "A: \n",
      " [[1. 0. 0.]\n",
      " [5. 0. 2.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 3.]\n",
      " [4. 0. 9.]]\n",
      "\n",
      "User Features:\n",
      " [[ 2.21829477e-01  4.58445949e-02]\n",
      " [ 8.50288016e-01  3.86369035e-01]\n",
      " [-1.91514702e-17 -6.81541027e-19]\n",
      " [-3.88289052e-01  2.35719092e-01]\n",
      " [-2.77549248e-01  8.90535654e-01]]\n",
      "Singular Values:\n",
      " [ 3.89366418 10.99269663]\n",
      "Item Features:\n",
      " [[ 8.63729488e-01 -7.45693935e-17 -5.03955724e-01]\n",
      " [ 5.03955724e-01 -7.49197375e-18  8.63729488e-01]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csc_matrix # It creates a sparse matrix\n",
    "from scipy.sparse.linalg import svds # It performs svd on the matrix\n",
    "\n",
    "# Matrix A rep utility matrix with users as rows & items as columns\n",
    "# Sparse matrix A has some predefined user-item ratings\n",
    "\n",
    "A = csc_matrix([\n",
    "    [1,0,0], # user 1 ratings\n",
    "    [5,0,2], # user 2 ratings\n",
    "    [0,1,0], # user 3 ratings\n",
    "    [0,0,3], # user 4 ratings\n",
    "    [4,0,9]], dtype=float) # user 5 ratings\n",
    "\n",
    "# perform SVD on matrix A, svds decomposes A into 3 matrices: U, Sigma (Σ), and VT\n",
    "# k=2 means we are reducing matrix to 2 latent factors\n",
    "\n",
    "u, s, vt = svds(A, k=2)\n",
    "print(u, s, vt)\n",
    "\n",
    "print(\" \")\n",
    "print(\"A: \\n\", A.toarray()) # convert sparse matrix to dense formatt for easy viewing\n",
    "print(\"\")\n",
    "\n",
    "# U reps user factors, s reps singular values & V.T reps item features\n",
    "print(f\"User Features:\\n {u}\") # shows user embeddings(latent factors for users)\n",
    "print(f\"Singular Values:\\n {s}\") # singular values in diagonal matrix Σ\n",
    "print(f\"Item Features:\\n {vt}\") # shows item embeddings(latent factors for items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Recreating The oringal Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation of rating matrix:\n",
      " [[ 1.00000000e+00 -6.81834071e-17  0.00000000e+00]\n",
      " [ 5.00000000e+00 -2.78699767e-16  2.00000000e+00]\n",
      " [-6.81834071e-17  5.61672411e-33  3.11086340e-17]\n",
      " [ 2.22044605e-16  9.33259021e-17  3.00000000e+00]\n",
      " [ 4.00000000e+00  7.24407782e-18  9.00000000e+00]]\n",
      " \n",
      "Rounded Approximation of rating matrix:\n",
      " [[ 1. -0.  0.]\n",
      " [ 5. -0.  2.]\n",
      " [-0.  0.  0.]\n",
      " [ 0.  0.  3.]\n",
      " [ 4.  0.  9.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We multiply the decomposed matrices U, Σ, and V.T\n",
    "# np.diag creates a diagonal matrix from singular values\n",
    "reconstructed_matrix = u.dot(np.diag(s).dot(vt))\n",
    "print(f\"Approximation of rating matrix:\\n {reconstructed_matrix}\")\n",
    "print(\" \")\n",
    "\n",
    "# Round off to get integer values, best for comparison\n",
    "print(f\"Rounded Approximation of rating matrix:\\n {np.round(reconstructed_matrix)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **C}** K- MEANS CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> OVERVIEW\n",
    "* It groups data into clusters where intra-class similarity{*Similarity among members of same grp*} is high & inter-class similarity{*Similarity of diff grps is low*} is low.\n",
    "* Similarity in terms of distance. The closer two points are the more similar they are.\n",
    "> TYPES OF CLUSTERING\n",
    "1. **Hierarchical Algorithm**: Builds clusters step by step either by i) Merging clusters or ii) Splitting clusters\n",
    "2. **Non-Hierarchical Algorithm(K-means)**: Divides data into pre-specified number of clusters, iteratively refining them.\n",
    "> KEY CONCEPTS & STEPS\n",
    "* K is predefined - nb of clusters are set beforehand.\n",
    "* **Centroid Update** : Clusters' centers are recalculated after each iterations.\n",
    "* K-means clustering also is an iterative algo that reaches for a pre-determined nb of clusters within an unbalanced dataset & follows the fwg steps:\n",
    "    * 1. Initialize K centroids\n",
    "    * 2. Assign each pt to the cluster which is closest.\n",
    "    * 3. Recompute the cluster centroids\n",
    "    * 4. Reassign the observations to one of the clusters according to some rule.\n",
    "    * 5. Stop if there is no reallocation\n",
    "> EVALUATION OF CLUSTER FITNESS\n",
    "1. **Variance Ratio {Calinski-Harabasz Score}**\n",
    "   * Evaluates cluster tightness & separation\n",
    "2. **Silhouette Score**\n",
    "    * Another metric for cluster fitness\n",
    "> ELBOW PLOT\n",
    "       \n",
    "**PURPOSE**: It helps to find the optimal value of k  \n",
    "**HOW TO INTERPRET**: Look for the elbow in the plot, where adding more clusters offers diminishing returns in cluster quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **D}** HIERARCHICAL AGGLOMERATIVE CLUSTERING {HAC}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> UNDERSTANDING HAC\n",
    "* **Agglomerative Clustering**: Starts with each data pt as its own cluster & merges the closest clusters iteratively until only one remains or a stopping criterion {like a specific nb of clusters is met}\n",
    "> LINKING SIMILAR CLUSTERS\n",
    "* Linkage Criteria determines how to measure closest clusters\n",
    "* Scikit-Learn provides 3 linkage criteria:\n",
    "  * 1. **WARD**: \n",
    "      * Picks 2 clusters to merge in a way that the variance within all clusters increases the least leading to fairly equally sized.\n",
    "      * Minimizes variance within clusters{Creates equally sized clusters}\n",
    "  * 2. **AVERAGE**:\n",
    "      * Merges 2 clusters that have the smallest average distance btn all the pts.\n",
    "      * Minimizes the average distance btn all pts in 2 clusters.\n",
    "  * 3. **COMPLETE / MAXIMUM LINKAGE**\n",
    "      * Merges 2 clusters that have the smallest maximum distance btn their pts.\n",
    "      * Minimizes the maximum distance btn pts in 2 clusters.\n",
    "> VISUALIZATION TOOLS\n",
    "* **DENDOGRAM**: It visualizes the hierarchy of clusters.\n",
    "                This helps determine the nbs of clusters by cutting at a desired distance.\n",
    "\n",
    "| HAC  | K-means |\n",
    "| ---- | ---- |\n",
    "| No predefined nb of clusters | Predefined nb of clusters |\n",
    "| Clusters within clusters | No subgroups within clusters |\n",
    "| Represented as a hierarchy | Clusters shift iteratively | \n",
    "\n",
    "> APPLICATION\n",
    "* Market Segmentation, Photo Albums: \n",
    "    * HAC is used to organize hierarchical data like grouping photos by date, location & content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **E}** COMMON PROBLEMS WITH K-MEANS & HAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A} K-MEANS CLUSTERING  \n",
    "\n",
    "| ADVANTAGES | DISADVANTAGES |\n",
    "| -------   | ------- |\n",
    "| Easy to implement | Choosing right value of k is crucial & challenging when unknown |\n",
    "| Faster than HAC when dealing with many features | Sensitive to data scaling |\n",
    "| Produce tight clusters as centroids move | Heavily influenced by initial positions of centroids. Poor initialization leads to bad clustering results |\n",
    "|    | Multiple runs with diff initializations are needed to avoid bad results |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> B} HIERARCHICAL AGGLOMERATIVE CLUSTERING {HAC} \n",
    "\n",
    "| ADVANTAGES | DISADVANTAGES |\n",
    "| ---- | ---- |\n",
    "| Provides ordered structure of clusters that are useful for visualizations { i.e Dendograms} | Results depend on the distance metrics used |\n",
    "| Allows granular analysis, zooming  into clusters at diff levels | Early mis-grouping of objects can't be corrected leading to incorrect clustering |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In both methods:\n",
    "\n",
    " * Visualizing high-dimensional data can be challenging.\n",
    " * Mistakes are harder to detect without proper metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **F}** SEMI-SUPERVISED LEARNING & LOOK-ALIKE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Look-Alike Models** : Expands potential customer bases i.e Used mostly in business & marketing analytics.\n",
    "* **Semi-supervised Learning**: Reduces the need for expensive human labeling making it practical where large amt of unlabeled data is available but only a small labeled set exists{deep learning}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. LOOK ALIKE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> OBJECTIVE\n",
    "* Identify the potential valuable customers by finding those who resemble already known valuable ones.\n",
    "> HOW IT WORKS\n",
    "* Clustering algos are mainly used to segment valuable customers.\n",
    "* A similarity metric {i.e Euclidean distance} compares new customers to known valuable customers.\n",
    "* Direct marketing & resources are aimed at these customers who \"look like\" the valuable ones.\n",
    "* This is a powerful method to prospect potential customers without detailed labeling or explicit data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. SEMI-SUPERVISED LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> OBJECTIVE\n",
    "* Uses a mix of labeled & pseudo-labeled data to train models.\n",
    "> HOW IT WORKS\n",
    "* Supervised models are first trained on a small labeled dataset.\n",
    "* The trained model is used to generate pseudo-labels for the much larger unlabeled dataset. \n",
    "* A new dataset is created by combining the real labeled data with pseudo-labeled data.\n",
    "* The model is restrained on comined dataset leading to better performance due to increased data volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CONSIDERATIONS\n",
    "1. **Look-Alike Models**\n",
    "* Check out for overgeneralized customer profiles\n",
    "* Ensure similarity metrics align with business objectives\n",
    "2. **Semi-Supervised Learning**\n",
    "* Test rigorously with holdout sets to avoid feedback loops & incorrect pseudo-labeling which degrades performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

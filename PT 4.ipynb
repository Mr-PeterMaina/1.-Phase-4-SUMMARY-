{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4: NATURAL LANGUAGE PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A}** NLP & WORD VECTORIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> INTRODUCTION\n",
    "* **Natural Language Processing**: It focuses on making computers understand and interact with human language\n",
    "* Modern NLP is driven by machine Learning\n",
    "> TEXT PREPROCESSING\n",
    "1. **TOKENIZATION**\n",
    "* Splitting text into individual words or tokens.\n",
    "2. **CLEANING**\n",
    "* Removing punctuation, converting all to lowercase & handling characters to prepare the text.\n",
    "3. **STOP WORDS REMOVAL**\n",
    "* Eliminating common but non-informative workds like 'the', 'of'\n",
    "> STEMMING VS LEMMATIZATION\n",
    "1. **STEMMING**\n",
    "* Reduces words to their root form by chopping off the suffixes.\n",
    "* i.e \"Studies\" becomes \"studi\"\n",
    "2. **LEMMATIZATION**\n",
    "* More sophisticated, converts words to their base form.\n",
    "* i.e \"Studies\" becomes \"Study\"\n",
    "> VECTORIZATION\n",
    "1. **BAG OF WORDS (COUNT VECTORIZATION)**\n",
    "* Counts the frequency of each word in a document ignoring their order.\n",
    "* when working with 1 document, we create a single vector where each element in the vector corresponds to the count of a unique word in the document.\n",
    "* When working with multiple documents, we would store everything in a DataFrame.\n",
    "2. **TF-IDF (TERM FREQUENCY - INVERSE DOCUMENT FREQUENCY)**\n",
    "* Weighs words based on their importance across a document collection.\n",
    "* This gives higher value to rare but meaningful termss.\n",
    "> APPLICATIONS\n",
    "1. **Text CLASSIFICATION & SPAM DETECTION**\n",
    "* Bayesian mmethods e.g Naive Bayes are often used to classify text {i.e spam filtering}\n",
    "2. **REAL WORLD NLP PRODUCTS**\n",
    "* From simple chatbots to complex systems like siri & Google Duplex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **B}** INTRODUCTION TO REGULAR EXPRESSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> INTRO\n",
    "* **Regex**: A powerful tool for pattern matching & text filtering, enabling quick searches & manipulations within text data.\n",
    "* Useful in: Web scraping & text data preprocessing in NLP.\n",
    "* It is essential during tokenization stage in NLP. It improves how words with apostrophes (they're) are split into tokens.\n",
    "> BASIC PATTERNS \n",
    "* \"re\" library in python is used to compile patterns & find matches within strings.\n",
    "> REGEX COMPONENTS\n",
    "* **RANGE**: Define groups of characters using square brackets {[A-Z for uppercase letters]}\n",
    "* **CHARACTER CLASSES**: Shortcut for common tasks   \n",
    "      * {\\d for digits, \\w for wor characters}\n",
    "* **Groups & Quantifiers**: Use parantheses for groups & curly braces for specifying repitition { (A-Z0-9){3}  }\n",
    "> EXAMPLE \n",
    "* A regex pattern to match basic email address\n",
    "*  '([A-Za-z]+)@([A-Za-z]+)\\.com'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **C}** REGULAR EXPRESSIONS - CODEALONG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. READING FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatiron School Cafe Menu\n",
      "\n",
      "\n",
      "Appetizers\n",
      "\n",
      "Nachos - $10\n",
      "Calamari - $12\n",
      "3 Cheese Platter - $8.75\n",
      "\n",
      "Entrees\n",
      "\n",
      "Chicken Sandwich - $16.95\n",
      "A fried chicken sandwich with lettuce, tomato, and mayo. Add cheese for $1.50\n",
      "\n",
      "Flatiron Steak - $22\n",
      "A prime cut of Flatiron Steak, cooked to your liking. Comes with a side of vegetables. Add a salad or cup of soup for $3\n",
      "\n",
      "Garden Salad - $14\n",
      "A salad with stuff from the garden on our roof. 3 types of dressing available.\n",
      "\n",
      "\n",
      "Want to place an order for delivery? Call us at (555) 123-8452!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open(r\"C:\\Users\\User\\Documents\\Moringa_labs\\PHASE 4\\1.-Phase-4-SUMMARY-\\DATA\\menu.txt\", \"r\") as f:\n",
    "    file = f.read()\n",
    "\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. FINDING DIGITS \n",
    "* Create a pattern to match all individual digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '0', '1', '2', '3', '8', '7', '5', '1', '6', '9', '5', '1', '5', '0', '2', '2', '3', '1', '4', '3', '5', '5', '5', '1', '2', '3', '8', '4', '5', '2']\n"
     ]
    }
   ],
   "source": [
    "pattern = \"\\d\"\n",
    "p = re.compile(pattern)\n",
    "digits = p.findall(file)\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  3. MATCHING PRICES \n",
    "* Modify patetrn to find dollar amounts by escaping the dollar sign(\\$) and also include digits\n",
    "*  **This approach captures the first digits only,  truncates $10 to $1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$1', '$1', '$8', '$1', '$1', '$2', '$3', '$1']\n"
     ]
    }
   ],
   "source": [
    "pattern = \"\\$\\d\"\n",
    "p = re.compile(pattern)\n",
    "digits = p.findall(file)\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. IMPROVING PRICE PATTERN \n",
    "* update pattern to capture more characters after the dollar sign.\n",
    "* **It worked for some, it left out any prices that have less than 3 characters after initial match i.e $10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$8.75', '$16.9', '$1.50']\n"
     ]
    }
   ],
   "source": [
    "pattern = \"\\$\\d.{3}\"\n",
    "p = re.compile(pattern)\n",
    "digits = p.findall(file)\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5. CREATE COMPREHENSIVE PRICE PATTERN \n",
    "* Use groups and ranges to match all prices including decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$10', '$12', '$8.75', '$16.95', '$1.50', '$22', '$3', '$14']\n"
     ]
    }
   ],
   "source": [
    "pattern = \"(\\$\\d+\\.?\\d*)\"\n",
    "p = re.compile(pattern)\n",
    "digits = p.findall(file)\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $ matches the dollar sign. \n",
    "    * It's a metacharacter in regex showing end of a string. We escape this using backslash ( \\ )\n",
    "2. \\d+ \n",
    "    * \\d any digit from 0-9\n",
    "    * + one or more  preceding elements\n",
    "    * \\d+ one or more digits[$1,$10,$123 ]\n",
    "3. \\ .? \n",
    "    * . matches a literal dot, a decimal point  \n",
    "    * we escape it coz (.) is a metacharacter\n",
    "4. \\d* \n",
    "    * matches zero or more digits after the decimal pt\n",
    "    * e.g 10. , 10.0, 10.99\n",
    "5. ()\n",
    "    * creates a capturing group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 6. EXTRACTING PHONE NUMBERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(555) 123-8452', '123-8452')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = \"(\\(\\d{3}\\) (\\d{3}-\\d{4}))\"\n",
    "p = re.compile(pattern)\n",
    "digits = p.findall(file)\n",
    "digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/d{3} matches the first 3 digits of the phone number.    \n",
    "/d{4} matches the last 4 digits of the number.   \n",
    "\\ ( matches opening parenthesis   \n",
    "\\ ) matches closing parenthesis   \n",
    "\" \" the space matches a space character   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **D}** BAG OF WORDS MODEL {BoW}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CONCEPT OF BoW MODEL\n",
    "* We treat text as a \"bag of words\" without considering grammr or word order.\n",
    "* Major focus on word presence & frequency of words in a document.\n",
    "* It creates a vocabulary{The unique words} from all documents then it reps each document as a vector whereby each dimension corresponds to a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> STEPS TO BUILD A BOW MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. TOKENIZATION\n",
    "* Break text into individual words{tokens}\n",
    "  EXAMPLE: \n",
    "    * I Love NLP and I love Machine Learning\n",
    "    It will be tokenized as:\n",
    "    * **[\"I\", \"love\", \"NLP\", \"and\", \"I\", \"love\", \"machine\", \"learning\"]**  \n",
    "2. BUILDING A VOCABULARY\n",
    "* The unique words across entire corpus(collection of documents) are identified to form a vocabulary.\n",
    "    VOCABULARY EXE:\n",
    "    * **[\"I\", \"love\", \"NLP\", \"and\", \"machine\", \"learning\"]** \n",
    "3. CREATING A VECTOR REPRESENTATION\n",
    "* Each sentence is then represented as a vector of word counts from the vocabulary we have.\n",
    "  EXAMPLE: \n",
    "  ~ VOCABULARY\n",
    "    * \"I love NLP and I love machine Learning\" \n",
    "  ~ VECTOR REPRESENTATION  \n",
    "    * [2,2,1,1,1,1]\n",
    "4. DOCUMENT TERM MATRIX\n",
    "* In multiple documents, BoW creates a document-term matrix where each row rep a document and each column rep a word from the vocabulary.\n",
    "* The cells in the matrix, show the count of how often that word appears in the document.  \n",
    "* EXAMPLE: We have 2 sentences  \n",
    "DOC 1. \"I love NLP\"  \n",
    "DOC 2. \"I love machine learning\"\n",
    "\n",
    "* THE DOCUMENT TERM MATRIX\n",
    "\n",
    "|-  | I | love | NLP | and | machine | learning |\n",
    "| --- | --- | --- | --- | --- |  --- | --- | \n",
    "|Doc 1 |1 | 1 | 1  | 0 | 0 |  0  | \n",
    "|Doc 2 | 1 | 1 | 0 | 0 | 1 | 1 |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Advantages and disadvantages of BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Advantages | Disadvantages |\n",
    "| ----       | ----        |\n",
    "| Simplicity: It is easy to implement & is straightforward on text representation | Ignores Context: It doesn't capture the order/ rltship btn words i.e \"I love NLP\" would be same to \"NLP love I\" | \n",
    "| Effective for text classification tasks | High dimensionality: AS vocabulary grows, feature space becomes large causing sparsity in document-term matrix |\n",
    "|  | Unable to capture semantics: BoW considers word frequencies & ignores the context behing the words |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Variations & Extensions of BoW\n",
    "1. TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "* It's an extension of BoW, it adjusts word counts based on how frequently they appear across documents.\n",
    "* common words across many documents{the,is} are given less importance.\n",
    "* Rare but meaningful words{machine, learning} are given more weight.\n",
    "2. N-grams\n",
    "* Considers combination of n consecutive words. (bigrams, trigrams) to capture word order information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **E}** NLP FEATURE ENGINEERING WITH NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NLP Feature techniques are:\n",
    "   1. **StopWord Removal**: It helps reduce noise\n",
    "   2. **Frequency distributions**: It provides insights into common words.\n",
    "   3. **Stemming & Lemmatization**: It reduces word forms for more meaningful analysisi\n",
    "   4. **Biggrams & N-grams**: It captures word dependencies.\n",
    "   5. **Mutual Information**: It quantifies word associations esp for biggrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. **STOPWORDS REMOVAL**\n",
    "* Stopwords are common words i.e \"a\" \"and\" \"but\" \"or\" that aren't so meaningful but oftenlt dominate text.\n",
    "* Removing stopwords reduces dimensionality of data & highlights meaningful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
      "['sample', 'text', 'demonstrate', 'stopword', 'removal']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Get the english stopwords & add punctuation\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "stopwords_list += list(string.punctuation)\n",
    "print(stopwords_list)\n",
    "\n",
    "# Tokenization & stopword removal example\n",
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(\"This is a sample text to demonstrate stopword removal.\")\n",
    "stopped_tokens = [word.lower() for word in tokens if word.lower() not in stopwords_list]\n",
    "print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. FREQUENCY DISTRIBUTIONS\n",
    "* It shows how often each word appears in a dataset.\n",
    "* This allows us to understand common terms used in corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sample', 2), ('text', 1), ('demonstrate', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "# Tokenize words from sample text\n",
    "tokens = ['sample', 'text', 'demonstrate', 'sample', 'word']\n",
    "\n",
    "# create freq distnibution\n",
    "freq_dist = FreqDist(tokens)\n",
    "\n",
    "# display most common words\n",
    "most_common = freq_dist.most_common(3)\n",
    "print(most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. STEMMING & LEMMATIZATION\n",
    "* **A] Stemming** reduces words to their root form by fwg a set of predefined rules. I.e \"running\" & \"run\" will become \"run\"\n",
    "* Commonly used stemmer is Porter Stemmer\n",
    "* **B] Lemmatization** maps words to their base forms using a dictionary.\n",
    "* I.e \"running\" becomes \"run \" & \"better\" becomes \"good\"\n",
    "* This improves accuracy thus *lemmatization* is more accurate than stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'ran', 'run']\n"
     ]
    }
   ],
   "source": [
    "# A] STEMMING\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "# Initialize the porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Example words\n",
    "words = [\"running\", \"ran\", \"runs\"]\n",
    "\n",
    "# stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foot', 'running']\n",
      "['foot', 'run']\n"
     ]
    }
   ],
   "source": [
    "# B] LEMMATIZATION\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example words\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in [\"feet\", \"running\"]]\n",
    "print(lemmatized_words)\n",
    "\n",
    "# Not working for running\n",
    "# WordnetLemmatizer requires a POS{Part of speech}tag to identify and lemmatize a word.\n",
    "pos_word = [(\"feet\",\"n\"), (\"running\", \"v\")]\n",
    "lemmatized_word = [lemmatizer.lemmatize(word,pos) for word,pos in pos_word]\n",
    "print(lemmatized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. BIGRAMS & N-GRAMS\n",
    "* **Bigran** is a pair of adjacent words.\n",
    "* They help capture word dependencies i.e {\"New York\" & \"San Francisco\"}\n",
    "* **N-grams** are sequences of n words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'dog'), ('dog', 'played'), ('played', 'outside')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "# Sentence exe\n",
    "sentence = \"the dog played outside\"\n",
    "\n",
    "# Tokenize & create bigrams\n",
    "tokens = word_tokenize(sentence)\n",
    "bigram_tokens = list(bigrams(tokens))\n",
    "print(bigram_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5. MUTUAL INFORMATION {MI} SCORE\n",
    "* **MI score** measures dependence btn 2 words in a bigram.\n",
    "* High MI score: Shows that 2 words are strongly correlated {i.e \"San Francisco\"}\n",
    "* **Pointwise MI**: measures how much info is contained in the bigram compared to the individual words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **F}** CONTEXT FREE GRAMMAR{CFG} & PART OF SPEECH(POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> INTRO\n",
    "* CFG is a set of rules that explain the structure of sentences at the syntactic level without regarding meaning.\n",
    "* Used in both NLP esp in POS Tagging & Linguistics\n",
    "> CFG rules for sentence structure\n",
    "* A sentence(S) = Noun Phrase(NP) + Verb Phrase(VP)\n",
    "* Noun Phrase(NP) = Determiner(Det) + Noun(N)\n",
    "* Verb Phrase(VP) = Verb(V) + Noun Phrase(NP)\n",
    "> POS Tagging \n",
    "* It assigns grammatical labels (i.e nouns,verb) to words.\n",
    "* It helps computers interpret sentence structure.\n",
    "> PARSE TREES\n",
    "* It visually reps how a sentence is broken down into grammatical components\n",
    "> CFG's IMPORTANCE\n",
    "* Help computers process human language by understanding sentence structures.\n",
    "* It interprets ambiguous sentences (\"run\" can be both noun & verb)\n",
    "> CFG EXE FOR: \"I SHOT AN ELEPHANT IN MY PYJAMAS\"\n",
    "* Sentences(S) -> Noun Phrase(NP) + Verb Phrase(VP)\n",
    "* Noun Phrase(NP) -> \"I\"\n",
    "* Verb Phrase(VP) -> \"shot\" + Noun Phrase (NP) + Prepositional Phrase(PP)\n",
    "* Prepositional Phrase(PP) -> \"in\" + Noun Phrase(NP)\n",
    "> POS TAGGING  \n",
    "* It uses NLP tools(NLTK) simplifies this by automatically assigning POS tags from predefined datasets. i.e Penn Treebank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### **G}** TEXT CLASSIFICATION\n",
    "* Involves preprocessing, cleaning & transforming text data into format suitable for ML models.\n",
    "> STEPS IN TEXT CLASSIFICATION\n",
    "* 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
